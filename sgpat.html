<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Pattern Recognition & Spectrograph Visualizer</title>
    <link rel="stylesheet" href="assets/css/base.css" />
    <style>
      body,
      html {
        background-color: black;
        touch-action: none;
        display: flex;
        justify-content: center;
        align-items: center;
        font-family: Arial, sans-serif;
      }
      #error-message {
        position: absolute;
        top: 20px;
        left: 20px;
        color: #ff0000;
        background: rgba(0, 0, 0, 0.7);
        padding: 10px;
        border-radius: 5px;
        z-index: 10;
      }
    </style>
  </head>
  <body>
    <a href="index.html" class="home-link">Back to Library</a>
    <div id="error-message" style="display: none"></div>
    <canvas id="glCanvas"></canvas>
    <script type="module">
      // Import the pattern recognition logic
      import PatternRecognizer from './assets/js/utils/patternRecognition.ts';
      import {
        initAudio,
        getFrequencyData,
      } from './assets/js/utils/audio-handler.ts';

      const canvas = document.getElementById('glCanvas');
      const gl = canvas.getContext('webgl');
      const ctx2d = canvas.getContext('2d');

      if (!gl) {
        displayError(
          'Unable to initialize WebGL. Your browser may not support it.'
        );
        throw new Error('WebGL not supported');
      }

      // Handle resizing
      function resizeCanvas() {
        canvas.width = window.innerWidth;
        canvas.height = window.innerHeight;
        gl.viewport(0, 0, gl.drawingBufferWidth, gl.drawingBufferHeight);
      }
      window.addEventListener('resize', resizeCanvas);
      resizeCanvas();

      // Shader sources
      const vertexShaderSource = `
            attribute vec4 a_position;
            void main() {
                gl_Position = a_position;
            }
        `;

      const fragmentShaderSource = `
            precision highp float;
            uniform float u_time;
            uniform vec2 u_resolution;
            uniform float u_audioData;
            uniform vec3 u_colorOffset;
            uniform vec2 u_touch;
            
            vec3 dreamyGradient(vec2 uv, float timeOffset) {
                vec3 color = vec3(0.4 + 0.4 * sin(uv.x * 10.0 + u_audioData * 6.0 + timeOffset),
                                  0.4 + 0.4 * cos(uv.y * 10.0 + u_audioData * 6.0 + timeOffset),
                                  0.5 + 0.4 * sin((uv.x + uv.y) * 8.0 + u_audioData * 6.0 + timeOffset));
                return color + u_colorOffset * 0.7;
            }
            
            void main() {
                vec2 uv = gl_FragCoord.xy / u_resolution.xy;
                uv -= 0.5;
                uv *= 2.0;
                
                float dist = distance(uv, u_touch);
                float ripple = sin(dist * 15.0 - u_time * 3.0) * 0.15;
                float bloom = 0.3 / (dist * dist + 0.25);
                uv += ripple;
                
                vec3 color = dreamyGradient(uv, u_time * 0.6) * u_audioData;
                color += vec3(0.4 + 0.4 * sin(u_time * 0.4 + u_audioData * 2.0), 
                              0.4 + 0.4 * cos(u_time * 0.5 + u_audioData * 3.0), 
                              0.5 + 0.4 * sin(u_time * 0.6 + u_audioData * 1.5));
                color += bloom * 0.6;
                
                gl_FragColor = vec4(color, 1.0);
            }
        `;

      function createShader(gl, type, source) {
        const shader = gl.createShader(type);
        gl.shaderSource(shader, source);
        gl.compileShader(shader);
        if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
          console.error(gl.getShaderInfoLog(shader));
          gl.deleteShader(shader);
          return null;
        }
        return shader;
      }

      // Create shaders
      const vertexShader = createShader(
        gl,
        gl.VERTEX_SHADER,
        vertexShaderSource
      );
      const fragmentShader = createShader(
        gl,
        gl.FRAGMENT_SHADER,
        fragmentShaderSource
      );

      const program = gl.createProgram();
      gl.attachShader(program, vertexShader);
      gl.attachShader(program, fragmentShader);
      gl.linkProgram(program);
      if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
        console.error(gl.getProgramInfoLog(program));
        gl.deleteProgram(program);
        throw new Error('Program failed to link');
      }

      gl.useProgram(program);

      // Set up position buffer
      const positionBuffer = gl.createBuffer();
      gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
      const positions = [
        -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0,
      ];
      gl.bufferData(
        gl.ARRAY_BUFFER,
        new Float32Array(positions),
        gl.STATIC_DRAW
      );

      const positionAttributeLocation = gl.getAttribLocation(
        program,
        'a_position'
      );
      gl.enableVertexAttribArray(positionAttributeLocation);
      gl.vertexAttribPointer(
        positionAttributeLocation,
        2,
        gl.FLOAT,
        false,
        0,
        0
      );

      // Get uniforms locations
      const resolutionUniformLocation = gl.getUniformLocation(
        program,
        'u_resolution'
      );
      const timeUniformLocation = gl.getUniformLocation(program, 'u_time');
      const audioDataUniformLocation = gl.getUniformLocation(
        program,
        'u_audioData'
      );
      const colorOffsetUniformLocation = gl.getUniformLocation(
        program,
        'u_colorOffset'
      );
      const touchUniformLocation = gl.getUniformLocation(program, 'u_touch');

      gl.uniform2f(resolutionUniformLocation, canvas.width, canvas.height);

      let time = 0.0;
      let audioData = 0.0;
      let colorOffset = [0.0, 0.0, 0.0];
      let touchPoint = [0.0, 0.0];
      let analyser;
      let patternRecognizer;

      async function setupAudio() {
        try {
          const result = await initAudio();
          analyser = result.analyser;
          patternRecognizer = new PatternRecognizer(analyser);
          hideError();
          getAudioData();
        } catch (err) {
          displayError(
            'Microphone access is required for the visualization to work. Please allow microphone access.'
          );
          console.error('Error capturing audio: ', err);
        }
      }

      function getAudioData() {
        requestAnimationFrame(getAudioData);
        if (analyser) {
          const dataArray = getFrequencyData(analyser);
          const average =
            dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
          audioData = average / 128.0;
          updateSpectrograph(dataArray);
          patternRecognizer.updatePatternBuffer();
          const detectedPattern = patternRecognizer.detectPattern();

          if (detectedPattern) {
            colorOffset = [1.0, 0.0, 0.0]; // Change color if a pattern is detected
          } else {
            colorOffset = [0.0, 0.0, 0.0];
          }
        }
      }

      // 2D Spectrograph rendering
      function updateSpectrograph(dataArray) {
        ctx2d.clearRect(0, 0, canvas.width, canvas.height);
        const gradient = ctx2d.createLinearGradient(
          0,
          0,
          canvas.width,
          canvas.height
        );
        gradient.addColorStop(0, '#ff6ec7');
        gradient.addColorStop(0.5, '#8e44ad');
        gradient.addColorStop(1, '#3498db');
        ctx2d.fillStyle = gradient;
        const barWidth = (canvas.width / dataArray.length) * 1.5;
        let barHeight;
        let x = 0;

        for (let i = 0; i < dataArray.length; i++) {
          barHeight = dataArray[i] / 2;
          ctx2d.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
          x += barWidth + 1;
        }
      }

      setupAudio();

      // Main render loop
      function render() {
        time += 0.02;
        gl.uniform1f(timeUniformLocation, time);
        gl.uniform1f(audioDataUniformLocation, audioData);
        gl.uniform3fv(colorOffsetUniformLocation, colorOffset);
        gl.uniform2fv(touchUniformLocation, touchPoint);

        gl.clear(gl.COLOR_BUFFER_BIT);
        gl.drawArrays(gl.TRIANGLES, 0, 6);

        requestAnimationFrame(render);
      }

      requestAnimationFrame(render);

      // Handle pointer input for interaction
      canvas.addEventListener('pointermove', (event) => {
        const x = (event.clientX / canvas.width) * 2.0 - 1.0;
        const y = -(event.clientY / canvas.height) * 2.0 + 1.0;
        touchPoint = [x, y];
      });

      // Show error messages
      function displayError(message) {
        const errorMessageElement = document.getElementById('error-message');
        errorMessageElement.textContent = message;
        errorMessageElement.style.display = 'block';
      }

      function hideError() {
        const errorMessageElement = document.getElementById('error-message');
        errorMessageElement.style.display = 'none';
        errorMessageElement.textContent = '';
      }
    </script>
  </body>
</html>
